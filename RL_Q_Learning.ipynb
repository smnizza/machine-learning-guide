{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYfl47lC/kx7jcvNqs8a3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smnizza/machine-learning-guide/blob/main/RL_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning – Pembelajaran Berbasis Nilai Q\n",
        "\n",
        "Q-Learning adalah algoritma **Reinforcement Learning** yang digunakan untuk menemukan kebijakan optimal dalam pengambilan keputusan berbasis nilai **Q**.\n",
        "\n",
        "## Konsep Dasar Q-Learning\n",
        "- **Agent**: Entitas yang belajar dari lingkungan.\n",
        "- **State (S)**: Kondisi tempat agent berada.\n",
        "- **Action (A)**: Pilihan yang bisa diambil oleh agent.\n",
        "- **Reward (R)**: Feedback dari lingkungan setelah agent melakukan aksi.\n",
        "- **Q-Table**: Tabel yang menyimpan nilai Q untuk setiap state-action pair.\n",
        "\n",
        "## Formula Q-Learning\n",
        "$$\n",
        "Q(s, a) = Q(s, a) + \\alpha [r + \\gamma \\max Q(s', a') - Q(s, a)]\n",
        "$$\n",
        "di mana:\n",
        "- $\\alpha$ = Learning Rate\n",
        "- $\\gamma$ = Discount Factor\n",
        "- $r$ = Reward\n",
        "- $s'$ = State berikutnya"
      ],
      "metadata": {
        "id": "kk6dkPZaH-Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementasi Q-Learning pada Lingkungan GridWorld\n",
        "Kita akan menggunakan OpenAI Gym untuk mensimulasikan lingkungan GridWorld."
      ],
      "metadata": {
        "id": "TUjVafg4IHfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Membuat lingkungan FrozenLake dari OpenAI Gym\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)  # Grid 4x4"
      ],
      "metadata": {
        "id": "Adtv1iM5IIeO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi Q-table dengan nilai nol\n",
        "Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1   # Learning rate\n",
        "gamma = 0.9   # Discount factor\n",
        "epsilon = 1.0 # Exploration-exploitation trade-off\n",
        "epsilon_decay = 0.99\n",
        "episodes = 1000"
      ],
      "metadata": {
        "id": "msvS_6FTIJxs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inisialisasi Parameter Q-Learning\n",
        "- **Q-table** digunakan untuk menyimpan nilai Q untuk setiap state-action pair.\n",
        "- **α (alpha)** mengontrol seberapa cepat pembelajaran.\n",
        "- **γ (gamma)** menentukan seberapa jauh reward masa depan dipertimbangkan.\n",
        "- **ε (epsilon)** mengontrol eksplorasi vs eksploitasi."
      ],
      "metadata": {
        "id": "2Y_LiWdDIKz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(episodes):\n",
        "    state = env.reset()[0]  # Reset lingkungan and extract the observation\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Pilih aksi (Eksplorasi atau Eksploitasi)\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()  # Eksplorasi\n",
        "        else:\n",
        "            action = np.argmax(Q_table[state])  # Eksploitasi\n",
        "\n",
        "        # Ambil aksi dan dapatkan feedback dari lingkungan\n",
        "        new_state, reward, done, truncated, info = env.step(action)\n",
        "        done = bool(done)\n",
        "\n",
        "\n",
        "        # Update Q-value berdasarkan rumus Q-Learning\n",
        "        Q_table[state, action] = Q_table[state, action] + alpha * \\\n",
        "            (reward + gamma * np.max(Q_table[new_state]) - Q_table[state, action])\n",
        "\n",
        "        state = new_state  # Pindah ke state berikutnya\n",
        "\n",
        "    # Kurangi epsilon untuk lebih banyak eksploitasi setelah eksplorasi cukup\n",
        "    epsilon *= epsilon_decay"
      ],
      "metadata": {
        "id": "5maLxgjKIL-r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training dengan Q-Learning\n",
        "- **Eksplorasi**: Agent mencoba aksi acak untuk menemukan solusi baru.\n",
        "- **Eksploitasi**: Agent memilih aksi terbaik berdasarkan Q-table.\n",
        "- **Epsilon Decay**: Semakin lama agent belajar, semakin sedikit eksplorasi."
      ],
      "metadata": {
        "id": "tusfUjqKIN_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi hasil pembelajaran\n",
        "total_reward = 0\n",
        "test_episodes = 100\n",
        "\n",
        "for _ in range(test_episodes):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(Q_table[state])  # Pilih aksi terbaik\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "print(f\"Persentase keberhasilan: {total_reward / test_episodes * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dV6jH1WIOrM",
        "outputId": "62a9f73a-70cd-4670-a40f-5f33702b3f16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persentase keberhasilan: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Hasil Q-Learning\n",
        "- Agent diuji dalam 100 episode.\n",
        "- Persentase keberhasilan menunjukkan seberapa baik agent mempelajari kebijakan optimal."
      ],
      "metadata": {
        "id": "qo3nqpc-IPrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kesimpulan\n",
        "- **Q-Learning** adalah metode pembelajaran berbasis nilai yang mencari kebijakan optimal dalam pengambilan keputusan.\n",
        "- Algoritma ini berguna untuk **pathfinding**, **robotics**, dan **game AI**.\n",
        "- Untuk masalah yang lebih kompleks, **Deep Q-Network (DQN)** digunakan.\n",
        "\n",
        "### **Pengembangan Lebih Lanjut**\n",
        "- **Deep Q-Learning (DQN)** untuk lingkungan yang lebih kompleks.\n",
        "- **Multi-Agent Q-Learning** untuk sistem dengan banyak agen."
      ],
      "metadata": {
        "id": "2b8mCJ8yIQrV"
      }
    }
  ]
}